{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this phase of the project, we focus on applying unsupervised learning techniques, specifically clustering, \n",
    "to uncover hidden patterns and groupings in the data. Clustering allows us to segment similar data points without \n",
    "relying on labeled outputs, which can be especially useful in recommendation systems where user preferences\n",
    " or item categories are not always clearly defined.\n",
    "\n",
    "The goal is to enhance the performance of our previous model by exploring whether meaningful clusters can guide\n",
    " or refine the recommendations. For instance, grouping users with similar behavior or preferences could help \n",
    " suggest more relevant items to new or existing users based on their cluster.\n",
    "\n",
    "We applied two clustering algorithms:\n",
    "- K-Means Clustering\n",
    "- Agglomerative Clustering\n",
    "\n",
    "These algorithms were selected because they represent two different approaches: partitioning-based and \n",
    "hierarchy-based clustering. Each was tested on the dataset after removing the class label to ensure a purely\n",
    "unsupervised analysis.\n",
    "\n",
    "Clustering Algorithms Used\n",
    "\n",
    "1. K-Means Clustering\n",
    "\n",
    "K-Means is a popular partitioning algorithm that divides the dataset into k non-overlapping clusters. It works by:\n",
    "- Randomly initializing k centroids,\n",
    "- Assigning each data point to the nearest centroid,\n",
    "- Recalculating the centroids as the average of all points in each cluster,\n",
    "- Repeating the process until the centroids stop changing or reach a maximum number of iterations.\n",
    "\n",
    "Why we used K-Means:\n",
    "\n",
    "- Simple and fast, especially on large datasets.\n",
    "- Efficient when clusters are spherical and evenly sized.\n",
    "- Easy to interpret and visualize.\n",
    "\n",
    "Performance on our dataset:\n",
    "- **Silhouette Coefficient:** 0.02317 \n",
    "  This low value suggests the clusters are not well-defined or are overlapping.\n",
    "\n",
    "---\n",
    "\n",
    "2. Agglomerative Clustering\n",
    "\n",
    "Agglomerative Clustering is a type of hierarchical clustering that starts by treating each data point as its own\n",
    "cluster and merges the closest pairs step by step until only one cluster remains or a stopping condition is met.\n",
    "\n",
    "Why we used Agglomerative Clustering:\n",
    "- Useful when we don’t know the optimal number of clusters in advance.\n",
    "- Can find nested patterns and works well with non-spherical data.\n",
    "- Doesn’t require centroid calculation, making it suitable for categorical or mixed data.\n",
    "\n",
    "Performance on our dataset:\n",
    "- **Silhouette Coefficient:** 0.01209 \n",
    "  This is even lower than K-Means, indicating very weak separation between clusters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
